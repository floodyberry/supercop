/*
 *  This file is part of Binary-library.
 *
 *  Binary-library is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation, either version 3 of the License, or
 *  any later version.
 *
 *  Foobar is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with Foobar.  If not, see <http://www.gnu.org/licenses/>.
*/

//#MULTIPLICATION_127#
void low_mul(elt c, elt a, elt b) {
	__m128i ma, mb, m0, m1;
	__m128i t0;
	
	ma = _mm_load_si128((__m128i *)a);
	mb = _mm_load_si128((__m128i *)b);

	MUL4(m1,m0,ma,mb);	
	REDUCE(t0,m1,m0);

	_mm_store_si128((__m128i *) c, m0);
}

//#MULTIPLICATION_254#
//Karatsuba method
void low_mul254(elt254 c, elt254 a, elt254 b) {
	__m128i maL, maH, mbL, mbH, mabL, mabH;
	__m128i m1, m0, m2, m3, m4, m5, t0;

	maL = _mm_load_si128((__m128i *) &a[0]);
	maH = _mm_load_si128((__m128i *) &a[2]);		
	mbL = _mm_load_si128((__m128i *) &b[0]);
	mbH = _mm_load_si128((__m128i *) &b[2]);
	mabL = _mm_xor_si128(maL, maH);
	mabH = _mm_xor_si128(mbL, mbH);
	
	MUL4(m1,m0,maL,mbL);	
	MUL4(m3,m2,maH,mbH);
	MUL4(m5,m4,mabL,mabH);
	
	//final sum
	m2 = _mm_xor_si128(m2, m0);
	m3 = _mm_xor_si128(m3, m1);
	m4 = _mm_xor_si128(m4, m0);
	m5 = _mm_xor_si128(m5, m1);

	REDUCE(t0,m3,m2);
	REDUCE(t0,m5,m4);

	_mm_store_si128((__m128i *) &c[0], m2);
	_mm_store_si128((__m128i *) &c[2], m4);
}

//#MULTIPLICATION_254 - NO REDUCTION#
void low_mul_nr254(elt254_x2 c, elt254 a, elt254 b) {
	__m128i maL, maH, mbL, mbH, mabL, mabH;
	__m128i m1, m0, m2, m3, m4, m5, t0, t1;

	maL = _mm_load_si128((__m128i *) &a[0]);
	maH = _mm_load_si128((__m128i *) &a[2]);
	mbL = _mm_load_si128((__m128i *) &b[0]);
	mbH = _mm_load_si128((__m128i *) &b[2]);
	mabL = _mm_xor_si128(maL, maH);
	mabH = _mm_xor_si128(mbL, mbH);

	MUL4(m1,m0,maL,mbL);
	MUL4(m3,m2,maH,mbH);
	MUL4(m5,m4,mabL,mabH);

	//final sum
	m2 = _mm_xor_si128(m2, m0);
	m3 = _mm_xor_si128(m3, m1);
	m4 = _mm_xor_si128(m4, m0);
	m5 = _mm_xor_si128(m5, m1);

	_mm_store_si128((__m128i *) &c[0], m2);
	_mm_store_si128((__m128i *) &c[2], m3);
	_mm_store_si128((__m128i *) &c[4], m4);
	_mm_store_si128((__m128i *) &c[6], m5);
}

//#MULTIPLICATION_254 TO a = u
//The result is given to the same register
void low_mul_a(elt254 a) {
	elt tmp;

	types_copy(tmp, &a[2]);
	low_add(&a[2], &a[2], &a[0]);
	types_copy(&a[0], tmp);	
}

//#MULTIPLICATION_254 TO a = u
//The result is given in another register
void low_mul_a_2(elt254 b, elt254 a) {
	low_add(&b[2], &a[2], &a[0]);
	types_copy(&b[0], &a[2]);	
}

//#MULTIPLICATION_254 TO (a + 1) = (u + 1)
//The result is given in another register
void low_mul_aplus1(elt254 b, elt254 a) {
	types_copy(&b[2], &a[0]);
	low_add(&b[0], &a[0], &a[2]);
}

//#SQUARING_127#
void low_sq(elt b, elt a) {	
	__m128i m0, m1, t0, sq, mask;
	
	sq = _mm_set_epi32(0x55545150, 0x45444140, 0x15141110, 0x05040100);
	mask = _mm_set_epi32(0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F);

	m0 = _mm_load_si128((__m128i *) a);

	SQUARE(m0, m1, sq, mask);
	REDUCESQUARE(t0,m1,m0);

	_mm_store_si128((__m128i *) b, m0);
}

//#SQUARING_254#
void low_sq254(elt254 b, elt254 a) {
	__m128i ma0, ma1, mb0, mb1;
	__m128i t0, sq, mask;
	
	sq = _mm_set_epi32(0x55545150, 0x45444140, 0x15141110, 0x05040100);
	mask = _mm_set_epi32(0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F);

	ma0 = _mm_load_si128((__m128i *) &a[0]);
	mb0 = _mm_load_si128((__m128i *) &a[2]);
	SQUARE(ma0, ma1, sq, mask);
	SQUARE(mb0, mb1, sq, mask);
	REDUCESQUARE(t0,ma1,ma0);	
	REDUCESQUARE(t0,mb1,mb0);

	ma0 = _mm_xor_si128(ma0, mb0);

	_mm_store_si128((__m128i *) &b[0], ma0);
	_mm_store_si128((__m128i *) &b[2], mb0);
}

//#SQUARING_254 - NO REDUCTION#
void low_sq_nr254(elt254_x2 b, elt254 a) {
	__m128i ma0, ma1, mb0, mb1;
	__m128i t0, t1, sq, mask;
	
	sq = _mm_set_epi32(0x55545150, 0x45444140, 0x15141110, 0x05040100);
	mask = _mm_set_epi32(0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F);	

	ma0 = _mm_load_si128((__m128i *) &a[0]);
	mb0 = _mm_load_si128((__m128i *) &a[2]);

	SQUARE(ma0, ma1, sq, mask);
	SQUARE(mb0, mb1, sq, mask);

	ma0 = _mm_xor_si128(ma0, mb0);
	ma1 = _mm_xor_si128(ma1, mb1);

	_mm_store_si128((__m128i *) &b[0] , ma0);
	_mm_store_si128((__m128i *) &b[2] , ma1);
	_mm_store_si128((__m128i *) &b[4] , mb0);
	_mm_store_si128((__m128i *) &b[6] , mb1);
}

//#MULTI_SQUARING_127#
void low_sqi(elt b, elt a, int times) {
	__m128i m0, m1, t0, sq, mask;
	int i;
	
	sq = _mm_set_epi32(0x55545150, 0x45444140, 0x15141110, 0x05040100);
	mask = _mm_set_epi32(0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F);

	m0 = _mm_load_si128((__m128i *) a);

	for(i=0;i<times;i++) {
		SQUARE(m0, m1, sq, mask);		
		REDUCESQUARE(t0,m1,m0);
	}

	_mm_store_si128((__m128i *) b , m0);
}

//#MULTI_SQUARING_127 6x#
void low_sqr06(elt b, elt a) {
	__m128i r0;
	elt_pt p;
	int i;	

	r0 = _mm_setzero_si128();

	for (i=0;i<16;i++) {
		p = tbl_sqr06[i][(a[0]>>(4*i))&0x0F];
		r0 = _mm_xor_si128(r0, *(__m128i *)(p));

		p = tbl_sqr06[i+16][(a[1]>>(4*i))&0x0F];
		r0 = _mm_xor_si128(r0, *(__m128i *)(p));
	}

	 _mm_store_si128((__m128i *)b, r0);
}

//#MULTI_SQUARING_127 12x#
void low_sqr12(elt b, elt a) {
	__m128i r0;
	elt_pt p;
	int i;	

	r0 = _mm_setzero_si128();

	for (i=0;i<16;i++) {
		p = tbl_sqr12[i][(a[0]>>(4*i))&0x0F];
		r0 = _mm_xor_si128(r0, *(__m128i *)(p));

		p = tbl_sqr12[i+16][(a[1]>>(4*i))&0x0F];
		r0 = _mm_xor_si128(r0, *(__m128i *)(p));
	}

	 _mm_store_si128((__m128i *)b, r0);
}

//#MULTI_SQUARING_127 24x#
void low_sqr24(elt b, elt a) {
	__m128i r0;
	elt_pt p;
	int i;	

	r0 = _mm_setzero_si128();

	for (i=0;i<16;i++) {
		p = tbl_sqr24[i][(a[0]>>(4*i))&0x0F];
		r0 = _mm_xor_si128(r0, *(__m128i *)(p));

		p = tbl_sqr24[i+16][(a[1]>>(4*i))&0x0F];
		r0 = _mm_xor_si128(r0, *(__m128i *)(p));
	}

	 _mm_store_si128((__m128i *)b, r0);
}

//#MULTI_SQUARING_127 48x#
void low_sqr48(elt b, elt a) {
	__m128i r0;
	elt_pt p;
	int i;	

	r0 = _mm_setzero_si128();

	for (i=0;i<16;i++) {
		p = tbl_sqr48[i][(a[0]>>(4*i))&0x0F];
		r0 = _mm_xor_si128(r0, *(__m128i *)(p));

		p = tbl_sqr48[i+16][(a[1]>>(4*i))&0x0F];
		r0 = _mm_xor_si128(r0, *(__m128i *)(p));
	}

	 _mm_store_si128((__m128i *)b, r0);
}

//#SQUARE_ROOT_254#
void low_sqrt254(elt254 b, elt254 a) {
	__m128i a0, aL, aH, uu0, uu1, vv0, vv1;
	__m128i ma, mb;
	ui64 uu[2], vv[2];
	
	__m128i perm = _mm_set_epi32(0x0F0D0B09, 0x07050301, 0x0E0C0A08, 0x06040200);
	__m128i sqrtL = _mm_set_epi32(0x33322322, 0x31302120, 0x13120302, 0x11100100);	
	__m128i sqrtH = _mm_set_epi32(0xCCC88C88, 0xC4C08480, 0x4C480C08, 0x44400400);
	__m128i maskL = _mm_set_epi32(0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F);	
	__m128i maskH = _mm_set_epi32(0xF0F0F0F0, 0xF0F0F0F0, 0xF0F0F0F0, 0xF0F0F0F0);

	ma = _mm_load_si128((__m128i *) &a[0]);
	mb = _mm_load_si128((__m128i *) &a[2]);
	ma = _mm_xor_si128(ma, mb);

	//Extraction of even (ae0) and odd (ao0) bits
  	uu0 = _mm_shuffle_epi8(ma, perm);
  	
	uu1 = _mm_and_si128(uu0, maskL);
	vv1 = _mm_and_si128(uu0, maskH);
	vv1 = _mm_srli_epi64(vv1, 4);
	
	uu1 = _mm_shuffle_epi8(sqrtL, uu1);
	vv1 = _mm_shuffle_epi8(sqrtH, vv1);
	
	uu0 = _mm_xor_si128(uu1, vv1);
	
	uu1 = _mm_and_si128(uu0, maskL);
	vv1 = _mm_and_si128(uu0, maskH);

	//Extraction of even (ae0) and odd (ao0) bits
  	a0 = _mm_shuffle_epi8(mb, perm);
  	
	aL = _mm_and_si128(a0, maskL);
	aH = _mm_and_si128(a0, maskH);
	aH = _mm_srli_epi64(aH, 4);	
	
	aL = _mm_shuffle_epi8(sqrtL, aL);
	aH = _mm_shuffle_epi8(sqrtH, aH);
	
	a0 = _mm_xor_si128(aL, aH);
	
	aL = _mm_and_si128(a0, maskL);
	aH = _mm_and_si128(a0, maskH);

	//Multiplication of odd vector to constant value sqrt(x)
	//sqrt(x) = x^64 + x^32
	uu0 = _mm_unpacklo_epi64(uu1, aL);
	uu1 = _mm_unpackhi_epi64(uu1, aL);
	vv0 = _mm_unpacklo_epi64(vv1, aH);
	vv1 = _mm_unpackhi_epi64(vv1, aH);

	uu1 = _mm_slli_epi64(uu1, 4);
	vv0 = _mm_srli_epi64(vv0, 4);

	uu0 = _mm_xor_si128(uu0, uu1);
	vv0 = _mm_xor_si128(vv0, vv1);

	uu0 = _mm_xor_si128(uu0, _mm_slli_epi64(vv0, 32)); //b2b0
	vv0 = _mm_xor_si128(vv0, _mm_srli_epi64(vv0, 32)); //b3b1

	aL = _mm_unpacklo_epi64(uu0, vv0);
	aH = _mm_unpackhi_epi64(uu0, vv0);

	_mm_store_si128((__m128i *) &b[0], aL);
	_mm_store_si128((__m128i *) &b[2], aH);
}

//#INVERSE_127#
//Itoh-Tsujii Algorithm with the following addition chain
//2-3-6-12-14-48-96-120-126
void low_inv(elt b, elt a) {
	elt tmp;
	elt a2_6, a2_24;

	//a^(2^2-2) * a = a^(2^2-1)
	low_sq(b, a);
	low_mul(b, b, a);

	//a^(2^3-2) * a = a^(2^3-1)
	low_sq(b, b);
	low_mul(b, b, a);

	//a^(2^6-2^3) * a^(2^3-1) = a^(2^6-1)
	low_sqi(a2_6, b, 3);
	low_mul(a2_6, a2_6, b);

	//a^(2^12-2^6) * a^(2^6-1) = a^(2^12-1)
	//low_sqi(b, a2_6, 6);
	low_sqr06(b, a2_6);
	low_mul(b, b, a2_6);
	
	//a^(2^24-2^12) * a^(2^12-1) = a^(2^24-1)
	low_sqr12(a2_24, b);
	low_mul(a2_24, a2_24, b);

	//a^(2^48-2^24) * a^(2^24-1) = a^(2^48-1)
	low_sqr24(b, a2_24);
	low_mul(b, b, a2_24);

	//a^(2^96-2^48) * a^(2^48-1) = a^(2^96-1)
	low_sqr48(tmp, b);
	low_mul(b, tmp, b);

	//a^(2^120-2^24) * a^(2^24-1) = a^(2^120-1)
	low_sqr24(b, b);
	low_mul(b, b, a2_24);

	//a^(2^126-2^6) * a^(2^6-1) = a^(2^126-1)
	//low_sqi(b, b, 6);
	low_sqr06(b, b);
	low_mul(b, b, a2_6);

	//a^(2^127-2)
	low_sq(b, b);
}

//#INVERSE_254#
void low_inv254(elt254 b, elt254 a) {
	elt ma, mb;
	elt ma2, mb2, mab, minv;
	
	ma[0] = a[0]; ma[1] = a[1];
	mb[0] = a[2]; mb[1] = a[3];

	low_sq(ma2, ma);
	low_sq(mb2, mb);
	low_mul(mab, mb, ma);

	mab[0] = mab[0] ^ mb2[0] ^ ma2[0];
	mab[1] = mab[1] ^ mb2[1] ^ ma2[1];

	//minv = (a^2 + b^2 + ba)^(-1)
	low_inv(minv, mab);
	
	//ma = a + b
	ma[0] = ma[0] ^ mb[0];
	ma[1] = ma[1] ^ mb[1];

	low_mul(&b[0], ma, minv);
	low_mul(&b[2], mb, minv);
}

//#HALF_TRACE_127#
void low_htr(elt b, elt a) {
	int i;
	elt_pt p;
	__m128i r0;
	unsigned char *tmp0, *tmp1;

	r0 = _mm_setzero_si128();

	tmp0 = (unsigned char*) &a[0]; tmp1 = (unsigned char*) &a[1];

	for (i=0;i<8;i++) {
		p = tbl_htr[i][*tmp0++];
		r0 = _mm_xor_si128(r0, *(__m128i *)(p));
		p = tbl_htr[i+8][*tmp1++];
		r0 = _mm_xor_si128(r0, *(__m128i *)(p));
	}

	_mm_store_si128((__m128i *)b, r0);
}

//#HALF_TRACE_254#
//WARNING!! This half-trace cannot be used outside the halving function!!!
//It has now the inverse of the trace
void low_htr254(elt254 b, elt254 a) {
	elt ma;
	int tr_ma;

	low_htr(&b[2], &a[2]);
	
	low_add(ma, &a[0], &b[2]);
	low_add(ma, ma, &a[2]);

	tr_ma = low_trHTR(ma);

	low_htr(&b[0], ma);
	b[2] = b[2] ^ tr_ma;
}

