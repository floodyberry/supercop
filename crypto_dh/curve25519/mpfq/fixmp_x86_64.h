/*
 * Library MPFq, package mp.
 * Multiprecision routines (small number of words) for x86_64. 
 * Automatically generated by perl/gen_mp_x86_64.pl 
 */

#ifndef __MP_X86_64_H__
#define __MP_X86_64_H__

#define HAVE_NATIVE_ADDMUL1_1 1
static void 
addmul1_1(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %2, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    addq    %%rcx, %1\n"
  : "+rm" (z[0]), "+rm" (z[1])
  : "rm" (x[0]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
}

#define HAVE_NATIVE_ADDMUL1_2 1
static void
addmul1_2(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %3, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    movq    %4, %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, %1\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    addq    %%rcx, %2\n"
  : "+m" (z[0]), "+m" (z[1]), "+m" (z[2])
  : "m" (x[0]), "m" (x[1]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
}

#define HAVE_NATIVE_ADDMUL1_3 1
static void
addmul1_3(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %4, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    movq    %5, %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %6, %%rax\n"
   "    addq    %%rcx, %1\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, %2\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    addq    %%rcx, %3\n"
  : "+m" (z[0]), "+m" (z[1]), "+m" (z[2]), "+m" (z[3])
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
}

#define HAVE_NATIVE_ADDMUL1_4 1
static void
addmul1_4(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %5, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    movq    %6, %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %7, %%rax\n"
   "    addq    %%rcx, %1\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %8, %%rax\n"
   "    addq    %%rcx, %2\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, %3\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    addq    %%rcx, %4\n"
  : "+m" (z[0]), "+m" (z[1]), "+m" (z[2]), "+m" (z[3]), "+m" (z[4])
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), "m" (x[3]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
}

#define HAVE_NATIVE_ADDMUL1_5 1
static void
addmul1_5(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    addq    %%rcx, 40(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_6 1
static void
addmul1_6(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    addq    %%rcx, 48(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_7 1
static void
addmul1_7(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    addq    %%rcx, 56(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_8 1
static void
addmul1_8(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    addq    %%rcx, 64(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_9 1
static void
addmul1_9(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    addq    %%rcx, 72(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_1 1
static void 
mul1_1(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %2, %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, %0\n"
   "    movq    %%rdx, %1\n"
  : "+rm" (z[0]), "+rm" (z[1])
  : "rm" (x[0]), [mult] "r" (c)
  : "%rax", "%rdx");
}

#define HAVE_NATIVE_MUL1_2 1
static void
mul1_2(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, 16(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_3 1
static void
mul1_3(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, 24(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_4 1
static void
mul1_4(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, 32(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_5 1
static void
mul1_5(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, 40(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_6 1
static void
mul1_6(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, 48(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_7 1
static void
mul1_7(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 48(%%rdi)\n"
   "    movq    %%rdx, 56(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_8 1
static void
mul1_8(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    movq    %%rcx, 48(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 56(%%rdi)\n"
   "    movq    %%rdx, 64(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_9 1
static void
mul1_9(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    movq    %%rcx, 48(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    movq    %%rcx, 56(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 64(%%rdi)\n"
   "    movq    %%rdx, 72(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADD_1 1
static void
add_1(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  *z = *x + *y;
}

#define HAVE_NATIVE_ADD_2 1
static void
add_2(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %2, %%rax\n"
    "   addq    %4, %%rax\n"
    "   movq    %%rax, %0\n"

    "   movq    %3, %%rax\n"
    "   adcq    %5, %%rax\n"
    "   movq    %%rax, %1\n"
  : "=m" (z[0]), "=m" (z[1])
  : "m" (x[0]), "m" (x[1]), "m" (y[0]), "m" (y[1])
  : "%rax");
}

#define HAVE_NATIVE_ADD_3 1
static void
add_3(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %3, %%rax\n"
    "   addq    %6, %%rax\n"
    "   movq    %%rax, %0\n"

    "   movq    %4, %%rax\n"
    "   adcq    %7, %%rax\n"
    "   movq    %%rax, %1\n"

    "   movq    %5, %%rax\n"
    "   adcq    %8, %%rax\n"
    "   movq    %%rax, %2\n"
  : "=m" (z[0]), "=m" (z[1]), "=m" (z[2])
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), "m" (y[0]), "m" (y[1]), "m" (y[2])
  : "%rax");
}

#define HAVE_NATIVE_ADD_4 1
static void
add_4(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %4, %%rax\n"
    "   addq    %8, %%rax\n"
    "   movq    %%rax, %0\n"

    "   movq    %5, %%rax\n"
    "   adcq    %9, %%rax\n"
    "   movq    %%rax, %1\n"

    "   movq    %6, %%rax\n"
    "   adcq    %10, %%rax\n"
    "   movq    %%rax, %2\n"

    "   movq    %7, %%rax\n"
    "   adcq    %11, %%rax\n"
    "   movq    %%rax, %3\n"
  : "=m" (z[0]), "=m" (z[1]), "=m" (z[2]), "=m" (z[3])
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), "m" (x[3]), "m" (y[0]), "m" (y[1]), "m" (y[2]), "m" (y[3])
  : "%rax");
}

#define HAVE_NATIVE_ADD_5 1
static void
add_5(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   addq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   adcq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   adcq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   adcq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   adcq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADD_6 1
static void
add_6(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   addq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   adcq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   adcq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   adcq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   adcq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"

    "   movq    40(%%rsi), %%rax\n"
    "   adcq    40(%%rdx), %%rax\n"
    "   movq    %%rax, 40(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADD_7 1
static void
add_7(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   addq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   adcq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   adcq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   adcq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   adcq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"

    "   movq    40(%%rsi), %%rax\n"
    "   adcq    40(%%rdx), %%rax\n"
    "   movq    %%rax, 40(%%rdi)\n"

    "   movq    48(%%rsi), %%rax\n"
    "   adcq    48(%%rdx), %%rax\n"
    "   movq    %%rax, 48(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADD_8 1
static void
add_8(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   addq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   adcq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   adcq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   adcq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   adcq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"

    "   movq    40(%%rsi), %%rax\n"
    "   adcq    40(%%rdx), %%rax\n"
    "   movq    %%rax, 40(%%rdi)\n"

    "   movq    48(%%rsi), %%rax\n"
    "   adcq    48(%%rdx), %%rax\n"
    "   movq    %%rax, 48(%%rdi)\n"

    "   movq    56(%%rsi), %%rax\n"
    "   adcq    56(%%rdx), %%rax\n"
    "   movq    %%rax, 56(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADD_9 1
static void
add_9(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   addq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   adcq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   adcq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   adcq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   adcq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"

    "   movq    40(%%rsi), %%rax\n"
    "   adcq    40(%%rdx), %%rax\n"
    "   movq    %%rax, 40(%%rdi)\n"

    "   movq    48(%%rsi), %%rax\n"
    "   adcq    48(%%rdx), %%rax\n"
    "   movq    %%rax, 48(%%rdi)\n"

    "   movq    56(%%rsi), %%rax\n"
    "   adcq    56(%%rdx), %%rax\n"
    "   movq    %%rax, 56(%%rdi)\n"

    "   movq    64(%%rsi), %%rax\n"
    "   adcq    64(%%rdx), %%rax\n"
    "   movq    %%rax, 64(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_SUB_1 1
static void
sub_1(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  *z = *x - *y;
}

#define HAVE_NATIVE_SUB_2 1
static void
sub_2(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %2, %%rax\n"
    "   subq    %4, %%rax\n"
    "   movq    %%rax, %0\n"

    "   movq    %3, %%rax\n"
    "   sbbq    %5, %%rax\n"
    "   movq    %%rax, %1\n"
  : "=m" (z[0]), "=m" (z[1])
  : "m" (x[0]), "m" (x[1]), "m" (y[0]), "m" (y[1])
  : "%rax");
}

#define HAVE_NATIVE_SUB_3 1
static void
sub_3(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %3, %%rax\n"
    "   subq    %6, %%rax\n"
    "   movq    %%rax, %0\n"

    "   movq    %4, %%rax\n"
    "   sbbq    %7, %%rax\n"
    "   movq    %%rax, %1\n"

    "   movq    %5, %%rax\n"
    "   sbbq    %8, %%rax\n"
    "   movq    %%rax, %2\n"
  : "=m" (z[0]), "=m" (z[1]), "=m" (z[2])
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), "m" (y[0]), "m" (y[1]), "m" (y[2])
  : "%rax");
}

#define HAVE_NATIVE_SUB_4 1
static void
sub_4(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %4, %%rax\n"
    "   subq    %8, %%rax\n"
    "   movq    %%rax, %0\n"

    "   movq    %5, %%rax\n"
    "   sbbq    %9, %%rax\n"
    "   movq    %%rax, %1\n"

    "   movq    %6, %%rax\n"
    "   sbbq    %10, %%rax\n"
    "   movq    %%rax, %2\n"

    "   movq    %7, %%rax\n"
    "   sbbq    %11, %%rax\n"
    "   movq    %%rax, %3\n"
  : "=m" (z[0]), "=m" (z[1]), "=m" (z[2]), "=m" (z[3])
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), "m" (x[3]), "m" (y[0]), "m" (y[1]), "m" (y[2]), "m" (y[3])
  : "%rax");
}

#define HAVE_NATIVE_SUB_5 1
static void
sub_5(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   subq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   sbbq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   sbbq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   sbbq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   sbbq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_SUB_6 1
static void
sub_6(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   subq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   sbbq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   sbbq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   sbbq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   sbbq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"

    "   movq    40(%%rsi), %%rax\n"
    "   sbbq    40(%%rdx), %%rax\n"
    "   movq    %%rax, 40(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_SUB_7 1
static void
sub_7(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   subq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   sbbq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   sbbq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   sbbq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   sbbq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"

    "   movq    40(%%rsi), %%rax\n"
    "   sbbq    40(%%rdx), %%rax\n"
    "   movq    %%rax, 40(%%rdi)\n"

    "   movq    48(%%rsi), %%rax\n"
    "   sbbq    48(%%rdx), %%rax\n"
    "   movq    %%rax, 48(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_SUB_8 1
static void
sub_8(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   subq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   sbbq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   sbbq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   sbbq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   sbbq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"

    "   movq    40(%%rsi), %%rax\n"
    "   sbbq    40(%%rdx), %%rax\n"
    "   movq    %%rax, 40(%%rdi)\n"

    "   movq    48(%%rsi), %%rax\n"
    "   sbbq    48(%%rdx), %%rax\n"
    "   movq    %%rax, 48(%%rdi)\n"

    "   movq    56(%%rsi), %%rax\n"
    "   sbbq    56(%%rdx), %%rax\n"
    "   movq    %%rax, 56(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_SUB_9 1
static void
sub_9(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   subq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   sbbq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   sbbq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   sbbq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   sbbq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"

    "   movq    40(%%rsi), %%rax\n"
    "   sbbq    40(%%rdx), %%rax\n"
    "   movq    %%rax, 40(%%rdi)\n"

    "   movq    48(%%rsi), %%rax\n"
    "   sbbq    48(%%rdx), %%rax\n"
    "   movq    %%rax, 48(%%rdi)\n"

    "   movq    56(%%rsi), %%rax\n"
    "   sbbq    56(%%rdx), %%rax\n"
    "   movq    %%rax, 56(%%rdi)\n"

    "   movq    64(%%rsi), %%rax\n"
    "   sbbq    64(%%rdx), %%rax\n"
    "   movq    %%rax, 64(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL_1 1
static void 
mul_1(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  umul_ppmm(z[1], z[0], x[0], y[0]);
}

#define HAVE_NATIVE_MUL_2 1
static void
mul_2(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, 16(%%rdi)\n"
   "	movq	$0, 24(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 24(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_3 1
static void
mul_3(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, 24(%%rdi)\n"
   "	movq	$0, 32(%%rdi)\n"
   "	movq	$0, 40(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 32(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 40(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_4 1
static void
mul_4(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, 32(%%rdi)\n"
   "	movq	$0, 40(%%rdi)\n"
   "	movq	$0, 48(%%rdi)\n"
   "	movq	$0, 56(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 40(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 48(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 56(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_5 1
static void
mul_5(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, 40(%%rdi)\n"
   "	movq	$0, 48(%%rdi)\n"
   "	movq	$0, 56(%%rdi)\n"
   "	movq	$0, 64(%%rdi)\n"
   "	movq	$0, 72(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 48(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 56(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 64(%%rdi)\n"
   "  ### x*y[4]\n"
   "    movq	32(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 32(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 72(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_6 1
static void
mul_6(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, 48(%%rdi)\n"
   "	movq	$0, 56(%%rdi)\n"
   "	movq	$0, 64(%%rdi)\n"
   "	movq	$0, 72(%%rdi)\n"
   "	movq	$0, 80(%%rdi)\n"
   "	movq	$0, 88(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 56(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 64(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 72(%%rdi)\n"
   "  ### x*y[4]\n"
   "    movq	32(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 32(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 80(%%rdi)\n"
   "  ### x*y[5]\n"
   "    movq	40(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 40(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 88(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_7 1
static void
mul_7(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 48(%%rdi)\n"
   "    movq    %%rdx, 56(%%rdi)\n"
   "	movq	$0, 64(%%rdi)\n"
   "	movq	$0, 72(%%rdi)\n"
   "	movq	$0, 80(%%rdi)\n"
   "	movq	$0, 88(%%rdi)\n"
   "	movq	$0, 96(%%rdi)\n"
   "	movq	$0, 104(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 64(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 72(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 80(%%rdi)\n"
   "  ### x*y[4]\n"
   "    movq	32(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 32(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 88(%%rdi)\n"
   "  ### x*y[5]\n"
   "    movq	40(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 40(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 96(%%rdi)\n"
   "  ### x*y[6]\n"
   "    movq	48(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 48(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 104(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_8 1
static void
mul_8(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    movq    %%rcx, 48(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 56(%%rdi)\n"
   "    movq    %%rdx, 64(%%rdi)\n"
   "	movq	$0, 72(%%rdi)\n"
   "	movq	$0, 80(%%rdi)\n"
   "	movq	$0, 88(%%rdi)\n"
   "	movq	$0, 96(%%rdi)\n"
   "	movq	$0, 104(%%rdi)\n"
   "	movq	$0, 112(%%rdi)\n"
   "	movq	$0, 120(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 72(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 80(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 88(%%rdi)\n"
   "  ### x*y[4]\n"
   "    movq	32(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 32(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 96(%%rdi)\n"
   "  ### x*y[5]\n"
   "    movq	40(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 40(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 104(%%rdi)\n"
   "  ### x*y[6]\n"
   "    movq	48(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 48(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 112(%%rdi)\n"
   "  ### x*y[7]\n"
   "    movq	56(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 56(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 112(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 120(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_9 1
static void
mul_9(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    movq    %%rcx, 48(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    movq    %%rcx, 56(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 64(%%rdi)\n"
   "    movq    %%rdx, 72(%%rdi)\n"
   "	movq	$0, 80(%%rdi)\n"
   "	movq	$0, 88(%%rdi)\n"
   "	movq	$0, 96(%%rdi)\n"
   "	movq	$0, 104(%%rdi)\n"
   "	movq	$0, 112(%%rdi)\n"
   "	movq	$0, 120(%%rdi)\n"
   "	movq	$0, 128(%%rdi)\n"
   "	movq	$0, 136(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 80(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 88(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 96(%%rdi)\n"
   "  ### x*y[4]\n"
   "    movq	32(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 32(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 104(%%rdi)\n"
   "  ### x*y[5]\n"
   "    movq	40(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 40(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 112(%%rdi)\n"
   "  ### x*y[6]\n"
   "    movq	48(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 48(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 112(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 120(%%rdi)\n"
   "  ### x*y[7]\n"
   "    movq	56(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 56(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 112(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 120(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 128(%%rdi)\n"
   "  ### x*y[8]\n"
   "    movq	64(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 64(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 112(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 120(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 128(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 136(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_SQR_1 1
static void 
sqr_1(mp_limb_t *z, const mp_limb_t *x)
{
  umul_ppmm(z[1], z[0], x[0], x[0]);
}

#define HAVE_NATIVE_SQR_2 1
static void 
sqr_2(mp_limb_t *z, const mp_limb_t *x)
{
  __asm__ volatile(
   "	movq    %1, %%r8\n"
   "	movq	%0, %%rdi\n"
   "	movq    (%%r8), %%rax\n"
   "	mulq    %%rax\n"
   "	movq    %%rax, (%%rdi)\n"
   "	movq    8(%%r8), %%rax\n"
   "	movq    %%rdx, %%r9\n"
   "	mulq    %%rax\n"
   "	movq    %%rax, %%r10\n"
   "	movq    (%%r8), %%rax\n"
   "	movq    %%rdx, %%r11\n"
   "	mulq    8(%%r8)\n"
   "	addq    %%rax, %%r9\n"
   "	adcq    %%rdx, %%r10\n"
   "	adcq    $0, %%r11\n"
   "	addq    %%rax, %%r9\n"
   "	movq	%%r9, 8(%%rdi)\n"
   "	adcq    %%rdx, %%r10\n"
   "	movq	%%r10, 16(%%rdi)\n"
   "	adcq    $0, %%r11\n"
   "	movq	%%r11, 24(%%rdi)\n"
  : "+m" (z)
  : "m" (x) 
  : "%rax", "%rdx", "%rdi", "%r8", "%r9", "%r10", "%r11", "memory");
} 

#define HAVE_NATIVE_SQR_3 1
static void 
sqr_3(mp_limb_t *z, const mp_limb_t *x)
{
  __asm__ volatile(
   "	movq	%1, %%rsi\n"
   "	movq	%0, %%rdi\n"
   "	### diagonal elements\n"
   "	movq    (%%rsi), %%rax\n"
   "	mulq	%%rax\n"
   "	movq    %%rax, (%%rdi)\n"
   "	movq    8(%%rsi), %%rax\n"
   "	movq    %%rdx, 8(%%rdi)\n"
   "	mulq	%%rax\n"
   "	movq    %%rax, 16(%%rdi)\n"
   "	movq	16(%%rsi), %%rax\n"
   "	movq    %%rdx, 24(%%rdi)\n"
   "	mulq    %%rax\n"
   "    movq    %%rax, 32(%%rdi)\n"
   "    movq    %%rdx, 40(%%rdi)\n"
   "	### precompute triangle\n"
   "	### x[0]*x[1,2]\n"
   "	movq	(%%rsi), %%rcx\n"
   "	movq	8(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	movq	%%rax, %%r8\n"
   "	movq	%%rdx, %%r9\n"
   "	movq    16(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	addq	%%rax, %%r9\n"
   "	adcq	$0, %%rdx\n"
   "	movq	%%rdx, %%r10\n"
   "	### x[1]*x[2]\n"
   "	movq	8(%%rsi), %%rax\n"
   "	mulq	16(%%rsi)\n"
   "	addq	%%rax, %%r10\n"
   "	adcq	$0, %%rdx\n"
   "	### Shift triangle\n"
   "	addq	%%r8, %%r8\n"
   "	adcq	%%r9, %%r9\n"
   "	adcq	%%r10, %%r10\n"
   "	adcq	%%rdx, %%rdx\n"
   "	adcq	$0, 40(%%rdi)\n"
   "	### add shifted triangle to diagonal\n"
   "	addq	%%r8, 8(%%rdi)\n"
   "	adcq	%%r9, 16(%%rdi)\n"
   "	adcq	%%r10, 24(%%rdi)\n"
   "	adcq	%%rdx, 32(%%rdi)\n"
   "	adcq	$0, 40(%%rdi)\n"
   : "+m" (z)
   : "m" (x) 
   : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_SQR_4 1
static void 
sqr_4(mp_limb_t *z, const mp_limb_t *x)
{
  __asm__ volatile(
   "	movq	%1, %%rsi\n"
   "	movq	%0, %%rdi\n"
   "	### diagonal elements\n"
   "	movq    (%%rsi), %%rax\n"
   "	mulq	%%rax\n"
   "	movq    %%rax, (%%rdi)\n"
   "	movq    8(%%rsi), %%rax\n"
   "	movq    %%rdx, 8(%%rdi)\n"
   "	mulq	%%rax\n"
   "	movq    %%rax, 16(%%rdi)\n"
   "	movq	16(%%rsi), %%rax\n"
   "	movq    %%rdx, 24(%%rdi)\n"
   "	mulq    %%rax\n"
   "    movq    %%rax, 32(%%rdi)\n"
   "	movq	24(%%rsi), %%rax\n"
   "    movq    %%rdx, 40(%%rdi)\n"
   "	mulq    %%rax\n"
   "    movq    %%rax, 48(%%rdi)\n"
   "    movq    %%rdx, 56(%%rdi)\n"
   "	### precompute triangle\n"
   "	### x[0]*x[1:3]\n"
   "	movq	(%%rsi), %%rcx\n"
   "	movq	8(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	movq	%%rax, %%r8\n"
   "	movq	%%rdx, %%r9\n"
   "	movq    16(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	addq	%%rax, %%r9\n"
   "	adcq	$0, %%rdx\n"
   "	movq	%%rdx, %%r10\n"
   "	movq    24(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	addq	%%rax, %%r10\n"
   "	adcq	$0, %%rdx\n"
   "	movq	%%rdx, %%r11\n"
   "	### x[1]*x[2:3]\n"
   "	movq	8(%%rsi), %%rcx\n"
   "	movq	16(%%rsi), %%rax\n"
   "	xorq	%%r12, %%r12\n"
   "	mulq	%%rcx\n"
   "	addq	%%rax, %%r10\n"
   "	adcq	%%rdx, %%r11\n"
   "	adcq	$0, %%r12\n"
   "	movq	24(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	addq    %%rax, %%r11\n"
   "	adcq	$0, %%rdx\n"
   "	addq    %%rdx, %%r12\n"
   "	### x[2]*x[3]\n"
   "	movq	16(%%rsi), %%rax\n"
   "	mulq	24(%%rsi)\n"
   "	addq	%%rax, %%r12\n"
   "	adcq	$0, %%rdx\n"
   "	### Shift triangle\n"
   "	addq	%%r8, %%r8\n"
   "	adcq	%%r9, %%r9\n"
   "	adcq	%%r10, %%r10\n"
   "	adcq	%%r11, %%r11\n"
   "	adcq	%%r12, %%r12\n"
   "	adcq	%%rdx, %%rdx\n"
   "	adcq	$0, 56(%%rdi)\n"
   "	### add shifted triangle to diagonal\n"
   "	addq	%%r8, 8(%%rdi)\n"
   "	adcq	%%r9, 16(%%rdi)\n"
   "	adcq	%%r10, 24(%%rdi)\n"
   "	adcq	%%r11, 32(%%rdi)\n"
   "	adcq	%%r12, 40(%%rdi)\n"
   "	adcq	%%rdx, 48(%%rdi)\n"
   "	adcq	$0, 56(%%rdi)\n"
   : "+m" (z)
   : "m" (x) 
   : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10",
   "%r11", "%r12", "memory");
}

#endif  /* __MP_X86_64_H__ */
