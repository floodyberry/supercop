/*
 * Library MPFq, package mp.
 * Multiprecision routines (small number of words) for x86_32. 
 * Automatically generated by perl/gen_mp_x86_32.pl 
 */

#ifndef __MP_X86_32_H__
#define __MP_X86_32_H__

#define HAVE_NATIVE_ADDMUL1_1 1
static void 
addmul1_1(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movl    %2, %%eax\n"
   "    mull    %[mult]\n"
   "    addl    %%eax, %0\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"
   "    addl    %%ecx, %1\n"
  : "+rm" (z[0]), "+rm" (z[1])
  : "rm" (x[0]), [mult] "r" (c)
  : "%eax", "%ecx", "%edx");
}

#define HAVE_NATIVE_ADDMUL1_2 1
static void
addmul1_2(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movl    %0, %%edi\n"
   "    movl    %1, %%esi\n"
   "    movl    (%%esi), %%eax\n"
   "    mull    %[mult]\n"
   "    addl    %%eax, (%%edi)\n"
   "    movl    4(%%esi), %%eax\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    addl    %%ecx, 4(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    addl    %%ecx, 8(%%edi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%eax", "%ecx", "%edx", "%esi", "%edi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_3 1
static void
addmul1_3(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movl    %0, %%edi\n"
   "    movl    %1, %%esi\n"
   "    movl    (%%esi), %%eax\n"
   "    mull    %[mult]\n"
   "    addl    %%eax, (%%edi)\n"
   "    movl    4(%%esi), %%eax\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    8(%%esi), %%eax\n"
   "    addl    %%ecx, 4(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    addl    %%ecx, 8(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    addl    %%ecx, 12(%%edi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%eax", "%ecx", "%edx", "%esi", "%edi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_4 1
static void
addmul1_4(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movl    %0, %%edi\n"
   "    movl    %1, %%esi\n"
   "    movl    (%%esi), %%eax\n"
   "    mull    %[mult]\n"
   "    addl    %%eax, (%%edi)\n"
   "    movl    4(%%esi), %%eax\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    8(%%esi), %%eax\n"
   "    addl    %%ecx, 4(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    12(%%esi), %%eax\n"
   "    addl    %%ecx, 8(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    addl    %%ecx, 12(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    addl    %%ecx, 16(%%edi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%eax", "%ecx", "%edx", "%esi", "%edi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_5 1
static void
addmul1_5(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movl    %0, %%edi\n"
   "    movl    %1, %%esi\n"
   "    movl    (%%esi), %%eax\n"
   "    mull    %[mult]\n"
   "    addl    %%eax, (%%edi)\n"
   "    movl    4(%%esi), %%eax\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    8(%%esi), %%eax\n"
   "    addl    %%ecx, 4(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    12(%%esi), %%eax\n"
   "    addl    %%ecx, 8(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    16(%%esi), %%eax\n"
   "    addl    %%ecx, 12(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    addl    %%ecx, 16(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    addl    %%ecx, 20(%%edi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%eax", "%ecx", "%edx", "%esi", "%edi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_6 1
static void
addmul1_6(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movl    %0, %%edi\n"
   "    movl    %1, %%esi\n"
   "    movl    (%%esi), %%eax\n"
   "    mull    %[mult]\n"
   "    addl    %%eax, (%%edi)\n"
   "    movl    4(%%esi), %%eax\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    8(%%esi), %%eax\n"
   "    addl    %%ecx, 4(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    12(%%esi), %%eax\n"
   "    addl    %%ecx, 8(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    16(%%esi), %%eax\n"
   "    addl    %%ecx, 12(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    20(%%esi), %%eax\n"
   "    addl    %%ecx, 16(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    addl    %%ecx, 20(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    addl    %%ecx, 24(%%edi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%eax", "%ecx", "%edx", "%esi", "%edi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_7 1
static void
addmul1_7(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movl    %0, %%edi\n"
   "    movl    %1, %%esi\n"
   "    movl    (%%esi), %%eax\n"
   "    mull    %[mult]\n"
   "    addl    %%eax, (%%edi)\n"
   "    movl    4(%%esi), %%eax\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    8(%%esi), %%eax\n"
   "    addl    %%ecx, 4(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    12(%%esi), %%eax\n"
   "    addl    %%ecx, 8(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    16(%%esi), %%eax\n"
   "    addl    %%ecx, 12(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    20(%%esi), %%eax\n"
   "    addl    %%ecx, 16(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    24(%%esi), %%eax\n"
   "    addl    %%ecx, 20(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    addl    %%ecx, 24(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    addl    %%ecx, 28(%%edi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%eax", "%ecx", "%edx", "%esi", "%edi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_8 1
static void
addmul1_8(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movl    %0, %%edi\n"
   "    movl    %1, %%esi\n"
   "    movl    (%%esi), %%eax\n"
   "    mull    %[mult]\n"
   "    addl    %%eax, (%%edi)\n"
   "    movl    4(%%esi), %%eax\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    8(%%esi), %%eax\n"
   "    addl    %%ecx, 4(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    12(%%esi), %%eax\n"
   "    addl    %%ecx, 8(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    16(%%esi), %%eax\n"
   "    addl    %%ecx, 12(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    20(%%esi), %%eax\n"
   "    addl    %%ecx, 16(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    24(%%esi), %%eax\n"
   "    addl    %%ecx, 20(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    28(%%esi), %%eax\n"
   "    addl    %%ecx, 24(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    addl    %%ecx, 28(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    addl    %%ecx, 32(%%edi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%eax", "%ecx", "%edx", "%esi", "%edi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_9 1
static void
addmul1_9(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movl    %0, %%edi\n"
   "    movl    %1, %%esi\n"
   "    movl    (%%esi), %%eax\n"
   "    mull    %[mult]\n"
   "    addl    %%eax, (%%edi)\n"
   "    movl    4(%%esi), %%eax\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    8(%%esi), %%eax\n"
   "    addl    %%ecx, 4(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    12(%%esi), %%eax\n"
   "    addl    %%ecx, 8(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    16(%%esi), %%eax\n"
   "    addl    %%ecx, 12(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    20(%%esi), %%eax\n"
   "    addl    %%ecx, 16(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    24(%%esi), %%eax\n"
   "    addl    %%ecx, 20(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    28(%%esi), %%eax\n"
   "    addl    %%ecx, 24(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    movl    32(%%esi), %%eax\n"
   "    addl    %%ecx, 28(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    mull    %[mult]\n"
   "    addl    %%eax, %%ecx\n"
   "    adcl    $0, %%edx\n"
   "    addl    %%ecx, 32(%%edi)\n"
   "    adcl    $0, %%edx\n"
   "    movl    %%edx, %%ecx\n"

   "    addl    %%ecx, 36(%%edi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%eax", "%ecx", "%edx", "%esi", "%edi", "memory");
}

#define HAVE_NATIVE_ADD_1 1
static void
add_1(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  *z = *x + *y;
}

#define HAVE_NATIVE_ADD_2 1
static void
add_2(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movl    %0, %%edi\n"
    "   movl    %1, %%esi\n"
    "   movl    %2, %%edx\n"
    "   movl    (%%esi), %%eax\n"
    "   addl    (%%edx), %%eax\n"
    "   movl    %%eax, (%%edi)\n"

    "   movl    4(%%esi), %%eax\n"
    "   adcl    4(%%edx), %%eax\n"
    "   movl    %%eax, 4(%%edi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%eax", "%edx", "%esi", "%edi", "memory");
}

#define HAVE_NATIVE_ADD_3 1
static void
add_3(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movl    %0, %%edi\n"
    "   movl    %1, %%esi\n"
    "   movl    %2, %%edx\n"
    "   movl    (%%esi), %%eax\n"
    "   addl    (%%edx), %%eax\n"
    "   movl    %%eax, (%%edi)\n"

    "   movl    4(%%esi), %%eax\n"
    "   adcl    4(%%edx), %%eax\n"
    "   movl    %%eax, 4(%%edi)\n"

    "   movl    8(%%esi), %%eax\n"
    "   adcl    8(%%edx), %%eax\n"
    "   movl    %%eax, 8(%%edi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%eax", "%edx", "%esi", "%edi", "memory");
}

#define HAVE_NATIVE_ADD_4 1
static void
add_4(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movl    %0, %%edi\n"
    "   movl    %1, %%esi\n"
    "   movl    %2, %%edx\n"
    "   movl    (%%esi), %%eax\n"
    "   addl    (%%edx), %%eax\n"
    "   movl    %%eax, (%%edi)\n"

    "   movl    4(%%esi), %%eax\n"
    "   adcl    4(%%edx), %%eax\n"
    "   movl    %%eax, 4(%%edi)\n"

    "   movl    8(%%esi), %%eax\n"
    "   adcl    8(%%edx), %%eax\n"
    "   movl    %%eax, 8(%%edi)\n"

    "   movl    12(%%esi), %%eax\n"
    "   adcl    12(%%edx), %%eax\n"
    "   movl    %%eax, 12(%%edi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%eax", "%edx", "%esi", "%edi", "memory");
}

#define HAVE_NATIVE_ADD_5 1
static void
add_5(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movl    %0, %%edi\n"
    "   movl    %1, %%esi\n"
    "   movl    %2, %%edx\n"
    "   movl    (%%esi), %%eax\n"
    "   addl    (%%edx), %%eax\n"
    "   movl    %%eax, (%%edi)\n"

    "   movl    4(%%esi), %%eax\n"
    "   adcl    4(%%edx), %%eax\n"
    "   movl    %%eax, 4(%%edi)\n"

    "   movl    8(%%esi), %%eax\n"
    "   adcl    8(%%edx), %%eax\n"
    "   movl    %%eax, 8(%%edi)\n"

    "   movl    12(%%esi), %%eax\n"
    "   adcl    12(%%edx), %%eax\n"
    "   movl    %%eax, 12(%%edi)\n"

    "   movl    16(%%esi), %%eax\n"
    "   adcl    16(%%edx), %%eax\n"
    "   movl    %%eax, 16(%%edi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%eax", "%edx", "%esi", "%edi", "memory");
}

#define HAVE_NATIVE_ADD_6 1
static void
add_6(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movl    %0, %%edi\n"
    "   movl    %1, %%esi\n"
    "   movl    %2, %%edx\n"
    "   movl    (%%esi), %%eax\n"
    "   addl    (%%edx), %%eax\n"
    "   movl    %%eax, (%%edi)\n"

    "   movl    4(%%esi), %%eax\n"
    "   adcl    4(%%edx), %%eax\n"
    "   movl    %%eax, 4(%%edi)\n"

    "   movl    8(%%esi), %%eax\n"
    "   adcl    8(%%edx), %%eax\n"
    "   movl    %%eax, 8(%%edi)\n"

    "   movl    12(%%esi), %%eax\n"
    "   adcl    12(%%edx), %%eax\n"
    "   movl    %%eax, 12(%%edi)\n"

    "   movl    16(%%esi), %%eax\n"
    "   adcl    16(%%edx), %%eax\n"
    "   movl    %%eax, 16(%%edi)\n"

    "   movl    20(%%esi), %%eax\n"
    "   adcl    20(%%edx), %%eax\n"
    "   movl    %%eax, 20(%%edi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%eax", "%edx", "%esi", "%edi", "memory");
}

#define HAVE_NATIVE_ADD_7 1
static void
add_7(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movl    %0, %%edi\n"
    "   movl    %1, %%esi\n"
    "   movl    %2, %%edx\n"
    "   movl    (%%esi), %%eax\n"
    "   addl    (%%edx), %%eax\n"
    "   movl    %%eax, (%%edi)\n"

    "   movl    4(%%esi), %%eax\n"
    "   adcl    4(%%edx), %%eax\n"
    "   movl    %%eax, 4(%%edi)\n"

    "   movl    8(%%esi), %%eax\n"
    "   adcl    8(%%edx), %%eax\n"
    "   movl    %%eax, 8(%%edi)\n"

    "   movl    12(%%esi), %%eax\n"
    "   adcl    12(%%edx), %%eax\n"
    "   movl    %%eax, 12(%%edi)\n"

    "   movl    16(%%esi), %%eax\n"
    "   adcl    16(%%edx), %%eax\n"
    "   movl    %%eax, 16(%%edi)\n"

    "   movl    20(%%esi), %%eax\n"
    "   adcl    20(%%edx), %%eax\n"
    "   movl    %%eax, 20(%%edi)\n"

    "   movl    24(%%esi), %%eax\n"
    "   adcl    24(%%edx), %%eax\n"
    "   movl    %%eax, 24(%%edi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%eax", "%edx", "%esi", "%edi", "memory");
}

#define HAVE_NATIVE_ADD_8 1
static void
add_8(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movl    %0, %%edi\n"
    "   movl    %1, %%esi\n"
    "   movl    %2, %%edx\n"
    "   movl    (%%esi), %%eax\n"
    "   addl    (%%edx), %%eax\n"
    "   movl    %%eax, (%%edi)\n"

    "   movl    4(%%esi), %%eax\n"
    "   adcl    4(%%edx), %%eax\n"
    "   movl    %%eax, 4(%%edi)\n"

    "   movl    8(%%esi), %%eax\n"
    "   adcl    8(%%edx), %%eax\n"
    "   movl    %%eax, 8(%%edi)\n"

    "   movl    12(%%esi), %%eax\n"
    "   adcl    12(%%edx), %%eax\n"
    "   movl    %%eax, 12(%%edi)\n"

    "   movl    16(%%esi), %%eax\n"
    "   adcl    16(%%edx), %%eax\n"
    "   movl    %%eax, 16(%%edi)\n"

    "   movl    20(%%esi), %%eax\n"
    "   adcl    20(%%edx), %%eax\n"
    "   movl    %%eax, 20(%%edi)\n"

    "   movl    24(%%esi), %%eax\n"
    "   adcl    24(%%edx), %%eax\n"
    "   movl    %%eax, 24(%%edi)\n"

    "   movl    28(%%esi), %%eax\n"
    "   adcl    28(%%edx), %%eax\n"
    "   movl    %%eax, 28(%%edi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%eax", "%edx", "%esi", "%edi", "memory");
}

#define HAVE_NATIVE_ADD_9 1
static void
add_9(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movl    %0, %%edi\n"
    "   movl    %1, %%esi\n"
    "   movl    %2, %%edx\n"
    "   movl    (%%esi), %%eax\n"
    "   addl    (%%edx), %%eax\n"
    "   movl    %%eax, (%%edi)\n"

    "   movl    4(%%esi), %%eax\n"
    "   adcl    4(%%edx), %%eax\n"
    "   movl    %%eax, 4(%%edi)\n"

    "   movl    8(%%esi), %%eax\n"
    "   adcl    8(%%edx), %%eax\n"
    "   movl    %%eax, 8(%%edi)\n"

    "   movl    12(%%esi), %%eax\n"
    "   adcl    12(%%edx), %%eax\n"
    "   movl    %%eax, 12(%%edi)\n"

    "   movl    16(%%esi), %%eax\n"
    "   adcl    16(%%edx), %%eax\n"
    "   movl    %%eax, 16(%%edi)\n"

    "   movl    20(%%esi), %%eax\n"
    "   adcl    20(%%edx), %%eax\n"
    "   movl    %%eax, 20(%%edi)\n"

    "   movl    24(%%esi), %%eax\n"
    "   adcl    24(%%edx), %%eax\n"
    "   movl    %%eax, 24(%%edi)\n"

    "   movl    28(%%esi), %%eax\n"
    "   adcl    28(%%edx), %%eax\n"
    "   movl    %%eax, 28(%%edi)\n"

    "   movl    32(%%esi), %%eax\n"
    "   adcl    32(%%edx), %%eax\n"
    "   movl    %%eax, 32(%%edi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%eax", "%edx", "%esi", "%edi", "memory");
}

#endif  /* __MP_X86_32_H__ */
